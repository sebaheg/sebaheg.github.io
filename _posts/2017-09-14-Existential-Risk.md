---
layout: post
title: Existential Risk
---
Imagine being on a spaceship travelling through the hostile space at 30 km/s. Outside, it is freezing cold. The hull of the spaceship is keeping you warm and protecting you from deadly ultraviolet radiation. On the spaceship there is a limited supply of water, food, fuel and other necessities. A responsible capten would do everything in her power keep the passengers safe. She would struggle to avoid collision with asteroids, explosions onboard, overheating of the spaceship, degradation of the protecting hull and depletion of the limited resources. The name of the spaceship is planet earth. Its protecting hull is the atmosphere, ozon layer and its magnetic field. 

To me, this analogy makes it clear how fragile and insignificant our life on earth is. This fact is excellently is captured by the speech ["Pale Blue Dot"](https://www.youtube.com/watch?v=kmP4Xzt0rN4) by Carl Sagen. Our life as a species is threatened by several existential risk. 

![GitHub Logo](/images/upcoming-existential-risks.png)

<p align="center">
  <img width="250" height="250" src="/images/upcoming-existential-risks.png">
</p>

Nick Boström, a philosopher that has been working on existential risks, defines them as:

<p class="message">
"One that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development."
</p>

The core is that an existential risk are those risks that threaten the entire future of humanity - not just the present generation, but all future generations. But Boströms scope of existential risk is a bit bigger, it also incorporates situations such as permanent stagnation, flawed realization and subsequent ruination. Basically, he argues that not fulfilling the highest potential of technological development constitutes an existential risk. I am not sure I agree with him. I think it would be alright for humanity to follow a path of where the full technological potential is not fulfilled if that state could be maintained and protected for human extinction. 


Despite their importance, issues surrounding human extinction risks and related hazards remain poorly understood. There are existential risks that emanate from nature, and there are those that emanate from human activities, and it seems that within a time perspective of a century or two, the latter dominate. What makes the situation especially intricate is that many of the risks seem to arise from possible future advances in the areas such as biotechnology, nanotechnology and artificial intelligence where potential gains are likewise enormous.COPY


## References
* Max Tegmark, Our Mathematical Universe: My Quest for the Ultimate Nature of Reality
* R. Buckmister Fuller, Operating Manual for Spaceship Earth
* Bill Gates, [Innovating to zero!](https://www.youtube.com/watch?v=JaF-fq2Zn7I)
* [existential-risk.org](http://www.existential-risk.org/)
* [Centre for the Study of Existential Risk](http://cser.org/)
* Wikipedia, [Global catastrophic risk](https://en.wikipedia.org/wiki/Global_catastrophic_risk)
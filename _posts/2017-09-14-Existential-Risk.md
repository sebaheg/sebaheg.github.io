---
layout: post
title: Existential Risk
---
Imagine being on a spaceship travelling through the hostile space at 30 km/s. Outside, it is freezing cold. The hull of the spaceship is keeping you warm and protecting you from deadly ultraviolet radiation. On the spaceship there is a limited supply of water, food, fuel and other necessities. A responsible capten would do everything in her power keep the passengers safe. She would struggle to avoid collision with asteroids, explosions onboard, overheating of the spaceship, degradation of the protecting hull and depletion of the limited resources. The name of the spaceship is planet earth. Its protecting hull is the atmosphere, ozon layer and its magnetic field. 

To me, this analogy makes it clear how fragile and insignificant our life on earth is. This fact is excellently is captured by the speech ["Pale Blue Dot"](https://www.youtube.com/watch?v=kmP4Xzt0rN4) by Carl Sagen. Our life as a species is threatened by several existential risk. To illustrate this, Max Tegmark made a graphic showing how life as we know it could be wiped out. 

![Existential Risks](/images/upcoming-existential-risks.png)

On the big time scales, we are concerned with astronomical and geological threats. These are for example the death of the sun, earth getting hit by asteroids and supervolcanoes causing volcanic winters. These are threats that are non antropogentic and hopefully we will be able to handle them with the technology available when the time comes. On the small time scales, the threats are post by humanity itself. They range from human activity causing climate change and loss of biodiversity to carelessly dealing with powerful future technology. In the light of these threats, I am not worried about getting hit by an asteroid. 

An existential catastropy is something that, by definition, never has happend in all of human history. Therefore, we can not us our old technique to learn from our mistakes - by then it is too late. In stead, we must develop technological foresight if we want to survive the century. Even though an event is unlikely, it is worth safeguarding against it if its concequences are devastating. This is referred to as the [precautionary principle](https://en.wikipedia.org/wiki/Precautionary_principle). Elon Musk applies this principle to climate change by calling the emission of greenhouse gases into the atmosphere ["The Dumbest Experiment in History"](https://www.youtube.com/watch?v=znukFtaWPAw). The same principle should be applied for powerful future technology such as general artificial intelligence (AGI), syntetic biology and molecular nanotechnology. 

In a wider perspective, an existential risk is defined as:

<p class="message">
"One that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development." - Nick Boström
</p>

The core is that an existential risk are those risks that threaten the entire future of humanity - not just the present generation, but all future generations. But Boströms scope of existential risk is a bit bigger, it also incorporates situations such as permanent stagnation, flawed realization and subsequent ruination. Basically, he argues that not fulfilling the highest potential of technological development constitutes an existential risk. I am not sure I agree with him. I think it would be alright for humanity to follow a path of where the full technological potential is not fulfilled if that state could be maintained and protected for human extinction. What do you think?

## References
* Max Tegmark, Our Mathematical Universe: My Quest for the Ultimate Nature of Reality
* R. Buckmister Fuller, Operating Manual for Spaceship Earth
* [existential-risk.org](http://www.existential-risk.org/)
* [Centre for the Study of Existential Risk](http://cser.org/)
* Wikipedia, [Global catastrophic risk](https://en.wikipedia.org/wiki/Global_catastrophic_risk)
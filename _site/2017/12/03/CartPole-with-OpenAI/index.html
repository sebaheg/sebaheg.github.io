<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Q-Learning for CartPole Problem &middot; El Niño
    
  </title>

  <!-- Include tags -->
  
    






  

  <!-- Enable Plotly -->
  

  <!-- Enable LaTeX/MathJax -->
  
    <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  

  <!-- Google analytics -->
  

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="192x192" href="/public/apple-touch-icon-precomposed.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/public/favicon.png">

  <!-- Social icons -->
  <link rel="stylesheet" href="/public/font-awesome/css/font-awesome.min.css">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="layout-reverse sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">

  <div class="sidebar-item">
    <p></p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">
      <i class="fa fa-home fa-fw" aria-hidden="true"></i> Home
    </a>

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">
            <i class="fa fa-fw fa-user-circle" aria-hidden="true"></i> About Me
          </a>
        
      
    
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/tags/">
            <i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags
          </a>
        
      
    

    <a class="sidebar-nav-item" href="/presentations/">
      <i class="fa fa-fw fa-comment" aria-hidden="true"></i> Presentations
    </a>

    <a class="sidebar-nav-item" href="https://www.linkedin.com/in/sebastian-haglund-el-gaidi-27a912b2/">
      <i class="fa fa-fw fa-linkedin"></i> LinkedIn
    </a>

    <a class="sidebar-nav-item" href="https://github.com/sebaheg">
      <i class="fa fa-fw fa-github"></i> GitHub
    </a>
  </nav>

  <div class="sidebar-item">
    <p>&copy; 2017. All rights reserved.</p>
  </div>

</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">El Niño</a>
            <small>Algorithms to Fight Global Warming</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Q-Learning for CartPole Problem</h1>
  <!-- Add date and comment counts -->
  <span class="post-date">03 Dec 2017
    <br>
    <!-- Add tags -->
    
    

    <ul class="tag-box inline">
     
      
      <li><a href="/tags/#machine-learning">machine-learning<span>6</span></a></li>
     
      
      <li><a href="/tags/#python">python<span>2</span></a></li>
     
    </ul>
  </span>

  <p>http://kengz.me/openai_lab/#openai-lab</p>

<p>We can do undiscounted markov reward processes if all sequences are finite.</p>

<p>We do not need to discount in this case?
sum is not infinite,
value is the same all the time</p>

<p>Read this:
http://kvfrans.com/simple-algoritms-for-solving-cartpole/
https://github.com/vmayoral/basic_reinforcement_learning/blob/master/tutorial4/README.md
https://keon.io/deep-q-learning/</p>

<p>Deep Q-learning
https://pythonprogramming.net/openai-cartpole-neural-network-example-machine-learning-tutorial/
https://www.pinchofintelligence.com/getting-started-openai-gym/
https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/
Here we talk more about Q-learning and the cartpole problem.</p>

<p>Reinforcement learning is a type of problem</p>

<p>OpenAI provides several different such problems (or environments) that can be accessed through a user-friendly API. in order to help people develop their RL algorithms.</p>

<p>https://gym.openai.com/</p>

<p>Idea: make a OpenAI gym for reinforcement learning for energy.
RL is a problem not a solution. It is a markov</p>

<p>Cartpole is a classical Reinforcement learning problem.
start with random actions and then learn how to master the game without being told how the game even works.
deep-q learning is a model-free approach as apposed to control theory/automatic control</p>

<p>In machine learning terms, CartPole is basically a binary classification problem. There are four features as inputs, which include the cart position, its velocity, the pole’s angle to the cart and its derivative (i.e. how fast the pole is “falling”).</p>

<p>The core of Q-Learning is to estimate a value for every possible pare of a state (s) and an action (a) by getting rewarded.</p>

<p>Basically, a (S, A)-tuple’s new q-value depends on its old q-value, the immediate reward received for the action and the maximum q-value achievable in the following state.</p>

<p>By repeatedly walking through all nodes and transistions, the agent can update any (S, A)-pairs q-value, while the results of good and bad actions are slowly “backpropagated” from terminal nodes to early nodes. The agent ends up with a (usually multidimensional) table mapping states and actions to q-values, so that given any state, the best action can be picked by choosing the highest respective q-value.</p>

<h3 id="openai-gym">OpenAI Gym</h3>
<p><a href="https://en.wikipedia.org/wiki/OpenAI#Universe">OpenAI</a></p>

<p>Can use OpenAI universe to make your own environments in the end.</p>

<h3 id="cartpole-problem">Cartpole problem</h3>
<p>A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart’s velocity.</p>

<p>The documentation for the problem can be found <a href="https://github.com/openai/gym/wiki/CartPole-v0">here</a>
For CartPole-v0 one of the actions applies force to the left, and one of them applies force to the right. (Can you figure out which is which?) So we affect force not speed directly!!</p>

<p>This is essentially the classic inverted pendulum problem which you could find in a typical undergraduate control course. However, instead of applying control theories, the goal here is to solve it using controlled trial-and-error, also known as reinforcement learning. COPY</p>

<p>The Cart-Pole world consists of a cart that moves along the horizontal axis and a pole that is anchored on the cart. At every time step, you can observe its position (x), velocity (x_dot), angle (theta), and angular velocity (theta_dot). These are the observable states of this world. At any state, the cart only has two possible actions: move to the left or move to the right.
In other words, the state-space of the Cart-Pole has four dimensions of continuous values and the action-space has one dimension of two discrete values.</p>

<table>
  <thead>
    <tr>
      <th>State variable</th>
      <th>Lower bound</th>
      <th>Upper bound</th>
    </tr>
  </thead>
  <tr>
    <th align="center">\( x \)</th>
    <th align="center">-4.8</th>
    <th align="center">4.8</th>
  </tr>
  <tr>
    <th align="center">\( \dot x \)</th>
    <th align="center">\( -\infty \)</th>
    <th align="center">\( \infty \)</th>
  </tr>
  <tr>
    <tr>
      <th align="center">\( \theta \)</th>
      <th align="center">-0.48</th>
      <th align="center">0.48</th>
    </tr>
  </tr>
  <tbody>
    <tr>
      <th align="center">\(\dot \theta \)</th>
      <th align="center">\( -\infty \)</th>
      <th align="center">\( \infty \)</th>
    </tr>
  </tbody>
</table>

<p>Num	Action
0	Push cart to the left
1	Push cart to the right</p>

<p>Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it</p>

<p>Starting state:
All observations are assigned a uniform random value between ±0.05</p>

<p>Episode Termination</p>

<p>Pole Angle is more than ±20.9°
Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)
Episode length is greater than 200
Solved Requirements</p>

<p>Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.</p>

<h3 id="random-actions">Random actions</h3>

<p align="center">
  <video controls="controls">
    <source type="video/mp4" src="/videos/im.mp4" />&lt;/source&gt;
    <p>Your browser does not support the video element.</p>
  </video>
</p>

<h3 id="q-learning">Q-Learning</h3>
<p>In here, we represent the world as a graph of states connected by transitions (or actions). It means that to predict your future state, you will only need to consider your current state and the action that you choose to perform. The key here is that you don’t need to consider your previous states. This is what people call a Markov Model. Although your past does have influences on your future, this model works because you can always encode information about the past in your current state.</p>

<p>Q-Learning is a method of finding these optimal policies. You can read more about it on this page. Essentially, through trials-and-errors, you find a Q-value for each state-action pair. This Q-value represents the desirability of an action given the current state. Over time, if the world is static (i.e. the physics or the cause-and-effects don’t change), the Q-values would converge and the optimal policy of a given state would be the action with the largest Q-value.</p>

<p>Let’s define a function Q(s, a) such that for given state s and action a it returns an estimate of a total reward we would achieve starting at this state, taking the action and then following some policy.</p>

<h3 id="discretizing-the-feature-space">Discretizing the feature space</h3>
<p>One major limitation of my classical Q-Learning approach was that the number of possible states had to be reduced from basically infinity (due to the observations’ continuous nature) to, in my case</p>

<p>Good reference to get started:
http://mnemstudio.org/path-finding-q-learning-tutorial.htm</p>

<p>Accordingly, with DQN we don’t need discrete buckets anymore, but are able to directly use the raw observations.</p>

<p>Having fewer optimal polices to find means faster training. However, discretizing the state-space too coarsely might prevent convergence as important information might be discretized away.</p>

<p>The input is a state-vector (or a batch of such) - consisting of four features in this case (which corresponds to four input neurons)The output is a vector of Q-values, one for each possible action - two in our case (corresponding to two output neurons).</p>

<h3 id="exploration-vs-exploitation">Exploration vs exploitation</h3>
<ul>
  <li>epsilon-greedy policy</li>
</ul>

<h3 id="training">Training</h3>
<p>experience tuples (old_state, performed_action, received_reward, new_state).
experience replay, go throught the game several times
At fixed intervals (e.g. after each training episode, but NOT after each step), batches are sampled from memory and used as training data for the network. Consequently, the network (hopefully) improves every episode and predicts more precise Q-values for state-action pairs.</p>

<ul>
  <li>try to increase the number of hidden units in the second hidden layer.</li>
  <li>Make mini-batch training instead of online training</li>
  <li>larger replay memory memory size to 100,000.</li>
  <li>Does it make sense to set gamma = 1?</li>
  <li>different weight decays</li>
  <li>tune the hyperparameters alpha, and so on.
logarithmic decay</li>
</ul>

<p>The purpose of the training is to enhance the ‘brain’ of our agent, represented by matrix Q.  More training results in a more optimized matrix Q</p>

<p>\begin{equation}
\varepsilon() = \max(0.01, \min(\varepsilon, 1-\log(t+1) \cdot decay))
\end{equation}</p>

<p><script type="math/tex">\max p</script>
<script type="math/tex">\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}</script></p>

<p>Actually the main challenge was to convert the continuous, 4-dimensional input space to a discrete space with a finite and preferably small, yet expressive, number of discrete states. The less states we have, the smaller the Q-table will be, the less steps the agent will need to properly learn its values. However, too few states might not hold enough information to properly represent the environment.</p>

<h3 id="hyperparameters">Hyperparameters</h3>
<p>alpha: regularization to make updates less radical, which, in the first place, prevents from errors caused by noise
epsilon: exploitation and exploration. This should prevent the algorithm from getting stuck in local minima. pick a random action with probability epsilon</p>

<p>OpenAI provides several different such problems (or environments) that can be accessed through a user-friendly API. in order to help people develop their RL algorithms.</p>

<p>https://gym.openai.com/
RL is a problem not a solution. It is a markov</p>

<p>Cartpole is a classical Reinforcement learning problem.
start with random actions and then learn how to master the game without being told how the game even works.
deep-q learning is a model-free approach as apposed to control theory/automatic control</p>

<p>In machine learning terms, CartPole is basically a binary classification problem. There are four features as inputs, which include the cart position, its velocity, the pole’s angle to the cart and its derivative (i.e. how fast the pole is “falling”).</p>

<p>The core of Q-Learning is to estimate a value for every possible pare of a state (s) and an action (a) by getting rewarded.</p>

<p>Basically, a (S, A)-tuple’s new q-value depends on its old q-value, the immediate reward received for the action and the maximum q-value achievable in the following state.</p>

<p>By repeatedly walking through all nodes and transistions, the agent can update any (S, A)-pairs q-value, while the results of good and bad actions are slowly “backpropagated” from terminal nodes to early nodes. The agent ends up with a (usually multidimensional) table mapping states and actions to q-values, so that given any state, the best action can be picked by choosing the highest respective q-value.</p>

<h3 id="cartpole-problem-1">Cartpole problem</h3>
<p>For CartPole-v0 one of the actions applies force to the left, and one of them applies force to the right. (Can you figure out which is which?) So we affect force not speed directly!!</p>

<p>This is essentially the classic inverted pendulum problem which you could find in a typical undergraduate control course. However, instead of applying control theories, the goal here is to solve it using controlled trial-and-error, also known as reinforcement learning. COPY</p>

<p>The Cart-Pole world consists of a cart that moves along the horizontal axis and a pole that is anchored on the cart. At every time step, you can observe its position (x), velocity (x_dot), angle (theta), and angular velocity (theta_dot). These are the observable states of this world. At any state, the cart only has two possible actions: move to the left or move to the right.
In other words, the state-space of the Cart-Pole has four dimensions of continuous values and the action-space has one dimension of two discrete values.</p>

<table>
  <thead>
    <tr>
      <th>State variable</th>
      <th>Lower bound</th>
      <th>Upper bound</th>
    </tr>
  </thead>
  <tr>
    <tr>
      <th align="center">\( \theta \)</th>
      <td>21</td>
      <td>23</td>
    </tr>
  </tr>
  <tbody>
    <tr>
      <th align="center">\(\dot \theta \)</th>
      <td>10</td>
      <td>11</td>
    </tr>
    <tr>
      <th align="center">\( x \)</th>
      <td>4</td>
      <td>3</td>
    </tr>
    <tr>
      <th align="center">\( \dot x \)</th>
      <th align="center">10</th>
      <th align="center">100</th>
    </tr>
  </tbody>
</table>

<h3 id="random-actions-1">Random actions</h3>

<p align="center">
  <video controls="controls">
    <source type="video/mp4" src="/videos/im.mp4" />&lt;/source&gt;
    <p>Your browser does not support the video element.</p>
  </video>
</p>

<h3 id="discretizing-the-feature-space-1">Discretizing the feature space</h3>
<p>One major limitation of my classical Q-Learning approach was that the number of possible states had to be reduced from basically infinity (due to the observations’ continuous nature) to, in my case</p>

<p>Good reference to get started:
http://mnemstudio.org/path-finding-q-learning-tutorial.htm</p>

<p>Accordingly, with DQN we don’t need discrete buckets anymore, but are able to directly use the raw observations.</p>

<p>Having fewer optimal polices to find means faster training. However, discretizing the state-space too coarsely might prevent convergence as important information might be discretized away.</p>

<p>The input is a state-vector (or a batch of such) - consisting of four features in this case (which corresponds to four input neurons)The output is a vector of Q-values, one for each possible action - two in our case (corresponding to two output neurons).</p>

<h3 id="training-1">Training</h3>
<p>experience tuples (old_state, performed_action, received_reward, new_state).
experience replay, go throught the game several times
At fixed intervals (e.g. after each training episode, but NOT after each step), batches are sampled from memory and used as training data for the network. Consequently, the network (hopefully) improves every episode and predicts more precise Q-values for state-action pairs.</p>

<ul>
  <li>try to increase the number of hidden units in the second hidden layer.</li>
  <li>Make mini-batch training instead of online training</li>
  <li>larger replay memory memory size to 100,000.</li>
  <li>Does it make sense to set gamma = 1?</li>
  <li>different weight decays</li>
  <li>tune the hyperparameters alpha, and so on.
logarithmic decay</li>
</ul>

<p>The purpose of the training is to enhance the ‘brain’ of our agent, represented by matrix Q.  More training results in a more optimized matrix Q</p>

<p>\begin{equation}
\varepsilon() = \max(0.01, \min(\varepsilon, 1-\log(t+1) \cdot decay))
\end{equation}</p>

<p><script type="math/tex">\max p</script>
<script type="math/tex">\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}</script></p>

<p>Actually the main challenge was to convert the continuous, 4-dimensional input space to a discrete space with a finite and preferably small, yet expressive, number of discrete states. The less states we have, the smaller the Q-table will be, the less steps the agent will need to properly learn its values. However, too few states might not hold enough information to properly represent the environment.</p>

<h3 id="hyperparameters-1">Hyperparameters</h3>
<p>alpha: regularization to make updates less radical, which, in the first place, prevents from errors caused by noise
epsilon: exploitation and exploration. This should prevent the algorithm from getting stuck in local minima. pick a random action with probability epsilon</p>

<h3 id="references">References</h3>
<p>OpenAI Gym, <a href="https://gym.openai.com/docs/">gym.openai.com/docs/</a>
https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff
https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947</p>

<h3 id="references-1">References</h3>
<p>OpenAI Gym, <a href="https://gym.openai.com/docs/">gym.openai.com/docs/</a>
https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff
https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947
Blog posts:</p>
<ul>
  <li><a href="https://medium.com/@gtnjuvin/my-journey-into-deep-q-learning-with-keras-and-gym-3e779cc12762">Gaetan Juvin</a>
http://koaning.io/hello-deepq.html
http://koaning.io/hello-deepq.html
http://kvfrans.com/simple-algoritms-for-solving-cartpole/
https://medium.com/@gtnjuvin/my-journey-into-deep-q-learning-with-keras-and-gym-3e779cc12762
https://towardsdatascience.com/reinforcement-learning-w-keras-openai-actor-critic-models-f084612cfd69</li>
</ul>


</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      

        <li>
          <h3>
            <a href="/2017/12/04/Reinforcement-Learning-TD-and-Q-Learning/">
              Reinforcement Learning: TD and Q-Learning
              <small>04 Dec 2017</small>
            </a>
          </h3>
        </li>

      
    
      

        <li>
          <h3>
            <a href="/2017/12/04/Reinforcement-Learning-MDP-and-Bellman-Equation/">
              Reinforcement Learning: MDP and Bellman Equation
              <small>04 Dec 2017</small>
            </a>
          </h3>
        </li>

      
    
      

        <li>
          <h3>
            <a href="/2017/12/03/Reinforcement-Learning-Intro/">
              Reinforcement Learning: Intro
              <small>03 Dec 2017</small>
            </a>
          </h3>
        </li>

      
    
  </ul>
</div>



      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>

    <script id="dsq-count-scr" src="//el-nino.disqus.com/count.js" async></script>
    
  </body>
</html>

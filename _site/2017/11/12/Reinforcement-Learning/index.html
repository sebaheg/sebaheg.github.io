<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Reinforcement Learning &middot; El Niño
    
  </title>

  <!-- Include tags -->
  
    






  

  <!-- Enable Plotly -->
  

  <!-- Enable LaTeX/MathJax -->
  
    <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  

  <!-- Google analytics -->
  

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="192x192" href="/public/apple-touch-icon-precomposed.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/public/favicon.png">

  <!-- Social icons -->
  <link rel="stylesheet" href="/public/font-awesome/css/font-awesome.min.css">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="layout-reverse sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  
  <div class="sidebar-item">
    <p></p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">
      <i class="fa fa-home fa-fw" aria-hidden="true"></i> Home
    </a>

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">
            <i class="fa fa-fw fa-user-circle" aria-hidden="true"></i> About Me
          </a>
        
      
    
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/tags/">
            <i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags
          </a>
        
      
    

    <a class="sidebar-nav-item" href="https://www.linkedin.com/in/sebastian-haglund-el-gaidi-27a912b2/">
      <i class="fa fa-fw fa-linkedin"></i> LinkedIn
    </a>

    <a class="sidebar-nav-item" href="https://github.com/sebaheg">
      <i class="fa fa-fw fa-github"></i> GitHub
    </a>
  </nav>

  <div class="sidebar-item">
    <p>&copy; 2017. All rights reserved.</p>
  </div>

</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">El Niño</a>
            <small>Algorithms to Fight Global Warming</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Reinforcement Learning</h1>
  <!-- Add date and comment counts -->
  <span class="post-date">12 Nov 2017
    <br>
    <!-- Add tags -->
    
    

    <ul class="tag-box inline">
     
      
      <li><a href="/tags/#machine-learning">machine-learning<span>4</span></a></li>
     
    </ul>
  </span>

  <p>Read this:
https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/</p>

<p>One of the most fundamental question for scientists across the globe has been – “How to learn a new skill?”. The desire to understand the answer is obvious – if we can understand this, we can enable human species to do things we might not have thought before. Alternately, we can train machines to do more “human” tasks and create true artificial intelligence.</p>

<p>Reinforcement learning is a type of problem</p>

<p>OpenAI provides several different such problems (or environments) that can be accessed through a user-friendly API. in order to help people develop their RL algorithms.</p>

<p>https://gym.openai.com/
RL is a problem not a solution. It is a markov</p>

<p>Cartpole is a classical Reinforcement learning problem.
start with random actions and then learn how to master the game without being told how the game even works.
deep-q learning is a model-free approach as apposed to control theory/automatic control</p>

<p>In machine learning terms, CartPole is basically a binary classification problem. There are four features as inputs, which include the cart position, its velocity, the pole’s angle to the cart and its derivative (i.e. how fast the pole is “falling”).</p>

<p>The core of Q-Learning is to estimate a value for every possible pare of a state (s) and an action (a) by getting rewarded.</p>

<p>Basically, a (S, A)-tuple’s new q-value depends on its old q-value, the immediate reward received for the action and the maximum q-value achievable in the following state.</p>

<p>By repeatedly walking through all nodes and transistions, the agent can update any (S, A)-pairs q-value, while the results of good and bad actions are slowly “backpropagated” from terminal nodes to early nodes. The agent ends up with a (usually multidimensional) table mapping states and actions to q-values, so that given any state, the best action can be picked by choosing the highest respective q-value.</p>

<h3 id="cartpole-problem">Cartpole problem</h3>
<p>For CartPole-v0 one of the actions applies force to the left, and one of them applies force to the right. (Can you figure out which is which?) So we affect force not speed directly!!</p>

<p>This is essentially the classic inverted pendulum problem which you could find in a typical undergraduate control course. However, instead of applying control theories, the goal here is to solve it using controlled trial-and-error, also known as reinforcement learning. COPY</p>

<p>The Cart-Pole world consists of a cart that moves along the horizontal axis and a pole that is anchored on the cart. At every time step, you can observe its position (x), velocity (x_dot), angle (theta), and angular velocity (theta_dot). These are the observable states of this world. At any state, the cart only has two possible actions: move to the left or move to the right.
In other words, the state-space of the Cart-Pole has four dimensions of continuous values and the action-space has one dimension of two discrete values.</p>

<table>
  <thead>
    <tr>
      <th>State variable</th>
      <th>Lower bound</th>
      <th>Upper bound</th>
    </tr>
  </thead>
  <tr>
    <tr>
      <th align="center">\( \theta \)</th>
      <td>21</td>
      <td>23</td>
    </tr>
  </tr>
  <tbody>
    <tr>
      <th align="center">\(\dot \theta \)</th>
      <td>10</td>
      <td>11</td>
    </tr>
    <tr>
      <th align="center">\( x \)</th>
      <td>4</td>
      <td>3</td>
    </tr>
    <tr>
      <th align="center">\( \dot x \)</th>
      <th align="center">10</th>
      <th align="center">100</th>
    </tr>
  </tbody>
</table>

<h3 id="random-actions">Random actions</h3>

<p align="center">
  <video controls="controls">
    <source type="video/mp4" src="/videos/im.mp4" />&lt;/source&gt;
    <p>Your browser does not support the video element.</p>
  </video>
</p>

<h3 id="q-learning">Q-Learning</h3>
<p>In here, we represent the world as a graph of states connected by transitions (or actions). It means that to predict your future state, you will only need to consider your current state and the action that you choose to perform. The key here is that you don’t need to consider your previous states. This is what people call a Markov Model. Although your past does have influences on your future, this model works because you can always encode information about the past in your current state.</p>

<p>Q-Learning is a method of finding these optimal policies. You can read more about it on this page. Essentially, through trials-and-errors, you find a Q-value for each state-action pair. This Q-value represents the desirability of an action given the current state. Over time, if the world is static (i.e. the physics or the cause-and-effects don’t change), the Q-values would converge and the optimal policy of a given state would be the action with the largest Q-value.</p>

<h3 id="discretizing-the-feature-space">Discretizing the feature space</h3>
<p>One major limitation of my classical Q-Learning approach was that the number of possible states had to be reduced from basically infinity (due to the observations’ continuous nature) to, in my case</p>

<p>Good reference to get started:
http://mnemstudio.org/path-finding-q-learning-tutorial.htm</p>

<p>Accordingly, with DQN we don’t need discrete buckets anymore, but are able to directly use the raw observations.</p>

<p>Having fewer optimal polices to find means faster training. However, discretizing the state-space too coarsely might prevent convergence as important information might be discretized away.</p>

<p>The input is a state-vector (or a batch of such) - consisting of four features in this case (which corresponds to four input neurons)The output is a vector of Q-values, one for each possible action - two in our case (corresponding to two output neurons).</p>

<h3 id="training">Training</h3>
<p>experience tuples (old_state, performed_action, received_reward, new_state).
experience replay, go throught the game several times
At fixed intervals (e.g. after each training episode, but NOT after each step), batches are sampled from memory and used as training data for the network. Consequently, the network (hopefully) improves every episode and predicts more precise Q-values for state-action pairs.</p>

<ul>
  <li>try to increase the number of hidden units in the second hidden layer.</li>
  <li>Make mini-batch training instead of online training</li>
  <li>larger replay memory memory size to 100,000.</li>
  <li>Does it make sense to set gamma = 1?</li>
  <li>different weight decays</li>
  <li>tune the hyperparameters alpha, and so on.
logarithmic decay</li>
</ul>

<p>The purpose of the training is to enhance the ‘brain’ of our agent, represented by matrix Q.  More training results in a more optimized matrix Q</p>

<p>\begin{equation}
\varepsilon() = \max(0.01, \min(\varepsilon, 1-\log(t+1) \cdot decay))
\end{equation}</p>

<p><script type="math/tex">\max p</script>
<script type="math/tex">\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}</script></p>

<p>Actually the main challenge was to convert the continuous, 4-dimensional input space to a discrete space with a finite and preferably small, yet expressive, number of discrete states. The less states we have, the smaller the Q-table will be, the less steps the agent will need to properly learn its values. However, too few states might not hold enough information to properly represent the environment.</p>

<h3 id="hyperparameters">Hyperparameters</h3>
<p>alpha: regularization to make updates less radical, which, in the first place, prevents from errors caused by noise
epsilon: exploitation and exploration. This should prevent the algorithm from getting stuck in local minima. pick a random action with probability epsilon</p>

<h3 id="references">References</h3>
<p>OpenAI Gym, <a href="https://gym.openai.com/docs/">gym.openai.com/docs/</a>
https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff
https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947</p>


</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      

        <li>
          <h3>
            <a href="/2017/11/11/Q-Learning/">
              Q-Learning for CartPole Problem
              <small>11 Nov 2017</small>
            </a>
          </h3>
        </li>

      
    
      

        <li>
          <h3>
            <a href="/2017/10/22/Nordic-Data-Science-and-Machine-Learning-Summit/">
              Nordic Data Science and Machine Learning Summit
              <small>22 Oct 2017</small>
            </a>
          </h3>
        </li>

      
    
      

        <li>
          <h3>
            <a href="/2017/10/21/Dayahead-Trading/">
              Day-ahead Trading
              <small>21 Oct 2017</small>
            </a>
          </h3>
        </li>

      
    
  </ul>
</div>



      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>

    <script id="dsq-count-scr" src="//el-nino.disqus.com/count.js" async></script>
    
  </body>
</html>
